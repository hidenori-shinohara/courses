\documentclass[12pt, psamsfonts]{amsart}

%-------Packages---------
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{theoremref}
\usepackage{graphicx}
\usepackage[bookmarks]{hyperref}

%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Id}{Id}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}

\begin{document}

\title{Math 620 Homework Due 9/5}
\author{Hidenori Shinohara}
\maketitle

\begin{exer}
  Prove that $\delta: V \times \cdots \times V \rightarrow \mathbb{F}$ is independent of choice of basis $\{ e_i \} \subset V$ up to non-zero scalar.
\end{exer}

\begin{proof}
  Let $\{ e_i \}, \{ f_i \}$ be two bases of $V$.
  Let $v_1, \cdots, v_n \in V$ be given.
  We must show if $\delta(v_1, \cdots, v_n) = 0$ with both of the bases, or nonzero with both of the bases.
  Suppose that $\delta(v_1, \cdots, v_n) \ne 0$ with one of the bases, and it is $0$ with the other basis.
  Without loss of generality, we assume that $\{ e_i \}$ gives a nonzero value.
  Let $n \times n$ matrices $(v^i_j), (w^i_j)$ be given such that
  \begin{align*}
    \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}
      &= \begin{bmatrix} v^1_1 & \cdots & v^n_1 \\ \vdots & \ddots & \vdots \\ v^n_1 & \cdots & v^n_n \end{bmatrix}
         \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix} \\
      &= \begin{bmatrix} w^1_1 & \cdots & w^n_1 \\ \vdots & \ddots & \vdots \\ w^n_1 & \cdots & w^n_n \end{bmatrix}
         \begin{bmatrix} f_1 \\ f_2 \\ \vdots \\ f_n \end{bmatrix}.
  \end{align*}
  Since $\delta(v_1, \cdots, v_n) \ne 0$ with $\{ e_i \}$, $\det(v_i^j) \ne 0$.
  Therefore, the matrix $(v_i^j)$ is invertible.

  \begin{align*}
    \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix}
      = \begin{bmatrix} v^1_1 & \cdots & v^n_1 \\ \vdots & \ddots & \vdots \\ v^n_1 & \cdots & v^n_n \end{bmatrix}^{-1}
         \begin{bmatrix} w^1_1 & \cdots & w^n_1 \\ \vdots & \ddots & \vdots \\ w^n_1 & \cdots & w^n_n \end{bmatrix}
         \begin{bmatrix} f_1 \\ f_2 \\ \vdots \\ f_n \end{bmatrix}.
  \end{align*}

  Let $A$ denote the product of the two matrices.
  Then $\det(A) = \det((v_i^j)^{-1}(w_i^j)) = \det(v_i^j)^{-1}\det(w_i^j) = 0$.
  This implies that the row space of $A$ has a dimension less than $n$.
  Therefore, $\{ e_1, \cdots, e_n \}$ cannot span $V$ whose dimension is $n$.

  This is a contradiction, so $\delta$ is independent of choice of basis up to nonzero scaling.
\end{proof}

\begin{exer}
  Show that $\{ e^{i_1} \otimes \cdots \otimes e^{i_k} \mid 1 \leq i_1, \cdots, i_k \leq n \}$ is a basis of $T^k(V^*)$.
  Find $\dim T^k(V^*)$.
\end{exer}

\begin{proof}
$ $
  \begin{itemize}
    \item
      Linearly independent?
      Suppose $\sum c_{i_1, \cdots, i_k} e^{i_1} \otimes \cdots \otimes e^{i_k} = 0$.
      Let $1 \leq j_1, \cdots, j_k \leq n$ be given.
      \begin{align*}
        &(\sum_{i_1, \cdots, i_k} c_{i_1, \cdots, i_k} e^{i_1} \otimes \cdots \otimes e^{i_k})(e_{j_1}, \cdots, e_{j_k}) = 0 \\
          &\implies \sum_{i_1, \cdots, i_k} c_{i_1, \cdots, i_k} (e^{i_1} \otimes \cdots \otimes e^{i_k})(e_{j_1}, \cdots, e_{j_k}) = 0 \\
          &\implies \sum_{i_1, \cdots, i_k} c_{i_1, \cdots, i_k} e^{i_1}(e_{j_1}) \cdots e^{i_k}(e_{j_k}) = 0 \\
          &\implies c_{j_1, \cdots, j_k} e^{j_1}(e_{j_1}) \cdots e^{j_k}(e_{j_k}) = 0 \\
          &\implies c_{j_1, \cdots, j_k} = 0.
      \end{align*}
      Therefore, each $c_{i_1, \cdots, i_k} = 0$.
    \item
      Span?
      Let $f \in T^k(V^*)$.
      We claim that $f = \sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})e^{i_1} \otimes \cdots \otimes e^{i_k}$.
      Let $v_1, \cdots, v_k \in V$ be given.
      Since $\{ e_1, \cdots, e_n \}$ is a basis of $V$, so each $v_i$ can be represented as $v_i = \sum_{j} c^j_ie_j$.
      \begin{align*}
        &(\sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})e^{i_1} \otimes \cdots \otimes e^{i_k})(v_1, \cdots, v_k) \\
          &= (\sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})e^{i_1} \otimes \cdots \otimes e^{i_k})(c^j_1e_j, \cdots, c^j_ke_j) \\
          &= \sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})[(e^{i_1} \otimes \cdots \otimes e^{i_k})(c^j_1e_j, \cdots, c^j_ke_j)] \\
          &= \sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})[(c^j_1e^{i_1}(e_j)) \cdots (c^j_ke^{i_k}(e_j))] \\
          &= \sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})[(c^{i_1}_1e^{i_1}(e_{i_1})) \cdots (c^{i_k}_ke^{i_k}(e_{i_k}))] \\
          &= \sum_{i_1, \cdots, i_k} f(e_{i_1}, \cdots, e_{i_k})c^{i_1} \cdots c^{i_k} \\
          &= \sum_{i_1, \cdots, i_k} f(c^{i_1}e_{i_1}, \cdots, c^{i_k}e_{i_k}) \\
          &= \sum_{i_1, \cdots, i_{k - 1}} (\sum_{i_k} f(c^{i_1}e_{i_1}, \cdots, c^{i_k}e_{i_k})) \\
          &= \sum_{i_1, \cdots, i_{k - 1}} f(c^{i_1}e_{i_1}, \cdots, c^{i_{k - 1}}e_{i_{k - 1}}, \sum_{i_k} c^{i_k}e_{i_k})) \\
          &= \sum_{i_1, \cdots, i_{k - 1}} f(c^{i_1}e_{i_1}, \cdots, c^{i_{k - 1}}e_{i_{k - 1}}, v_k) \\
          &  \vdots \\
          &= f(v_1, \cdots, v_k).
      \end{align*}
  \end{itemize}
  The dimension is $n^k$ because each $i_j$ can be any integer between $1$ and $n$.
\end{proof}

\begin{exer}
  Let $w \in \wedge^2 V^*$.
  \begin{itemize}
    \item
      Show that there exists a basis $\{ e_1, \cdots, e_n \}$ of $V$ with a dual basis $\{ e^1, \cdots, e^n \}$ of $V^*$ such that $w = e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m}$ for some $m \leq n / 2$.
    \item
      $w^l = w \wedge \cdots \wedge w \ne 0$ if and only if $l \leq m$.
  \end{itemize}
\end{exer}

\begin{proof}
  Let $V_1 = V$.
  We will pick vectors inductively.

  Suppose that we have $V_i$ for some $i \in \mathbb{N}$.
  If $\forall v, v' \in V_i, w(v, v')  = 0$, then we are done.
  Suppose otherwise.
  Then there must exist $v, v' \in V_i$ such that $w(v, v') = 1$.
  Let $e_{2i - 1} = v, e_{2i} = v'$.
  Let $V_{i + 1} = \{ v \in V \mid w(v, e_{2i - 1}) = w(v, e_{2i}) = 0 \}$.
  We will repeat this process with the $V_{i + 1}$.

  For each $i$, we claim that $\{ e_1, \cdots, e_{2i} \}$ is linearly independent.
  (To-Do)

  Since $V$ is an $n$-dimensional vector space, this process will terminate.
  If not, it would imply the existence of a linearly independent set with more than $n$ vectors.
  Since the set of all the vectors we found is linearly independent, it can be extended to form a basis of $V$.

  Let $\{ e_1, \cdots, e_n \}$ be a basis that we obtain by extending the linearly independent set of vectors we found.
  Let $m$ be chosen such that $2m$ is the number of vectors we found.
  Let $\{ e^1, \cdots e^n \}$ denote the dual basis of $\{ e_1, \cdots, e_n \}$.
  By Proposition 4.1., we know the existence of such a basis and that the dimension of such a basis is $n$.
  We claim that $w = e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m}$.

  Because $w$ and $e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m}$ are bilinear, it suffices to identify what $(e_i, e_j)$ gets mapped to for each $i, j$.
  Let $i, j \in \{ 1, \cdots, n \}$ be given.
  \begin{itemize}
    \item
      Case 1: The pair $(i, j)$ equals $(2l - 1, 2l)$ for some $l \in \{1, \cdots, m \}$.
      Then $w(e_{2l - 1}, e_{2l}) = 1$ because that is how we found $e_{2l - 1}, e_2l}$.
      On the other hand,
      \begin{align*}
        &(e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m})(e_{2l - 1}, e_{2l}) \\
          &= (e^1 \wedge e^2)(e_{2l - 1}, e_{2l}) + \cdots + (e^{2m - 1} \wedge e^{2m})(e_{2l - 1}, e_{2l}) \\
          &= 1.
      \end{align*}
    \item
      Case 2: The pair $(i, j)$ equals $(2l, 2l - 1)$ for some $l \in \{1, \cdots, m \}$.
      Since $w$ and $e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m}$ are both alternating, $w(e_i, e_j) = -w(e_j, e_i)$ and $(e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m})(e_i, e_j) = -(e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m})(e_j, e_i)$.
      Then, by Case 1, they both result in $-1$.
    \item
      Case 3: Any other cases.
%      Then $(e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m})(e_i, e_j) = 0$.
%      We claim that $w(e_i, e_j) = 0$.
%      If $e_i = e_j$, then $w(e_i, e_j) = -w(e_j, e_i) = -w(e_i, e_j)$, so $w(e_i, e_j) = 0$.
%      Suppose otherwise.
%      Without loss of generality, assume $i < j$.
%      If $2m < i < j$, then $w(e_i, e_j) = 0$ because of the terminating condition.
%      Suppose $i \leq 2m$.
%      Then $e_j$ was in 
%
%      Suppose $w(e_i, e_j) \ne 0$.
%      Then at least one of $e_i, e_j$ is in $\span \{ e_1, \cdots, e_{2m} \}$.
%      TODO*****************
  \end{itemize}
  Therefore, $w = e^1 \wedge e^2 + \cdots + e^{2m - 1} \wedge e^{2m}$.
\end{proof}

\begin{exer}
  $\omega \wedge \tau = (-1)^{kl}\tau \wedge \omega
\end{exer}

\begin{proof}
  \begin{align*}
    \omega \wedge \tau
      &= (e^{i_1} \wedge \cdots \wedge e^{i_k}) \wedge (e^{j_1} \wedge \cdots \wedge e^{j_k}) \\
      &= e^{i_1} \wedge \cdots \wedge e^{i_k} \wedge e^{j_1} \wedge \cdots \wedge e^{j_k} \\
      &= (-1) e^{i_1} \wedge \cdots \wedge e_{i_{k - 1}} \wedge e^{j_1} \wedge e^{i_k} \wedge e_{j_1} \wedge \cdots \wedge e^{i_k} \\
      &\vdots \\
      &= (-1)^k e^{j_1} \wedge e^{i_1} \wedge \cdots \wedge e_{i_k} \wedge e_{j_2} \wedge \cdots \wedge e^{i_k} \\
      &= (-1)^{2k} e^{j_1} \wedge e_{j_2} \wedge e^{i_1} \wedge \cdots \wedge e_{i_k} \wedge e_{j_3} \wedge \cdots \wedge e^{i_k} \\
      &\vdots \\
      &= (-1)^{kl} e^{j_1} \wedge \cdots \wedge e_{j_k} \wedge e^{i_1} \wedge \cdots \wedge e_{i_k} \\
      &= (-1)^{kl} \tau \wedge \wedge.
  \end{align*}
\end{proof}


\begin{exer}
  Prove that $\{ \partial_1, \cdots, \partial_n \}$ is a basis of $T_p \mathbb{R}^n$.
\end{exer}

\begin{proof}
  $ $
  \begin{itemize}
    \item
      Linearly independent?
      Let $c_1, \cdots, c_n \in \mathbb{R}$ be given.
      Suppose $c_1\partial_1 + \cdots + c_n\partial_n = 0$.
      Then $\forall i, 0 = (c_1\partial_1 + \cdots + c_n\partial_n)(x^i) = c_i\partial_i(x^i) = c_i$.
      Therefore, $c_i = 0$ for each $i$.
    \item
      Span?
      Let $\lambda \in T_p \mathbb{R}^n$ be given.
      We claim that $\lambda = \sum \lambda(x^i)\partial_i$.
      Let $f \in \mathscr{C}^{\infty}$.
      Then $f(x) = f(p) + \sum_i [\frac{\partial f}{\partial x^i}(p)(x^i - p^i) + g^i(x)(x^i - p^i])$ for some smooth functions $g^i$ by Taylor's formula with remainder.
      For each $i$, $g_i(p) = 0$.
      \begin{align*}
        \lambda(f)
          &= \lambda(f(p)) + \lambda(\sum_i [\frac{\partial f}{\partial x^i}(p)(x^i - p^i) + g^i(x)(x^i - p^i)]) \\
          &= \lambda(\sum_i [\frac{\partial f}{\partial x^i}(p)(x^i - p^i) + g^i(x)(x^i - p^i)]) \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)\lambda(x^i - p^i) + \sum_i \lambda(g^i(x)(x^i - p^i)) \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)\lambda(x^i - p^i) + \sum_i [\lambda(g^i(x))(p^i - p^i) + \lambda(x^i - p^i)g^i(p)] \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)\lambda(x^i - p^i) + \sum_i \lambda(x^i - p^i)g^i(p) \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)\lambda(x^i - p^i) \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)(\lambda(x^i) - \lambda(p^i)) \\
          &= \sum_i \frac{\partial f}{\partial x^i}(p)\lambda(x^i) \\
          &= \sum_i \partial_i(f)\lambda(x^i) \\
          &= \sum_i \lambda(x^i)\partial_i(f) \\
          &= (\sum_i \lambda(x^i)\partial_i)(f) \\
      \end{align*}
  \end{itemize}
\end{proof}

\begin{exer}
  Show that $\{ dx^1, \cdots, dx^n \}$ is a basis of $T_p^*\mathbb{R}^n$ that is dual to $\{ \frac{\partial}{\partial x^j} \}^n_{j = 1} \subset T_p\mathbb{R}^n$.
\end{exer}

\begin{proof}
$ $
  \begin{itemize}
    \item
      Dual?
      Let $i, j \in \{1, \cdots, n \}$.
      $dx^i(\frac{\partial}{\partial x^j}) = \frac{\partial}{\partial x^j} x^i$.
      The partial derivative of $x^i$ with respect to $x^j$ is 1 if $i = j$ and 0 otherwise.
      Thus $dx^i(\frac{\partial}{\partial x^j}) = \delta^i_j$.
    \item
      Linearly independent?
      Let $c_1, \cdots, c_n \in \mathbb{R}$ be given.
      Suppose that $c_1dx^1 + \cdots + c_ndx^n = 0$.
      For any $i \in \{ 1, \cdots, n \}$,
      \begin{align*} (c_1dx^1 + \cdots + c_ndx^n)(\partial_i) = 0
          &\implies c_1(dx^1(\partial_i)) + \cdots + c_n(dx^n(\partial_i)) = 0 \\
          &\implies c_1(\partial_i(x^1)) + \cdots + c_n(\partial_i(x^n)) = 0 \\
          &\implies c_i\partial_i(x^i) = 0 \\
          &\implies c_i = 0.
      \end{align*}
      Therefore, $c_1 = \cdots = c_n = 0$.
      Therefore, $\{ dx^1, \cdots, dx^n \}$ is indeed linearly independent.
    \item
      Span?
      Let $f \in T_p^*\mathbb{R}^n$ be given.
      We claim that $f = \sum_{i=1}^{n} f(\partial_i)dx^i$.
      Let $\sum_{i=1}^{n} c_i\partial_i \in T_p \mathbb{R}^n$ be given where $c_i$'s are in $\mathbb{R}$.
      (It makes sense to assume that every element in $T_p\mathbb{R}^n$ is in this form because we showed earlier that $\{ \partial_1, \cdots, \partial_n \}$ is a basis of $T_p \mathbb{R}^n$.)
      \begin{align*}
        (\sum_{i=1}^{n} f(\partial_i)dx^i)(\sum_{j=1}^{n} c_j\partial_j)
          &= \sum_{i=1}^{n} \big[f(\partial_i)dx^i(\sum_{j=1}^{n} c_j\partial_j)\big] \\
          &= \sum_{i=1}^{n} f(\partial_i) \big[\sum_{j=1}^{n} c_jdx^i(\partial_j)\big] \\
          &= \sum_{i=1}^{n} f(\partial_i) \big[\sum_{j=1}^{n} c_j\partial_j(x^i)\big] \\
          &= \sum_{i=1}^{n} f(\partial_i) c_i \\
          &= f(\sum_{i=1}^{n} c_i\partial_i).
      \end{align*}
  \end{itemize}
\end{proof}


\end{document}


